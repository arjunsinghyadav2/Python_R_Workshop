plot(add[,c(5,6,7)],col=add$LC)
plot(add,col=add$LC)
plot(add,col=add$SS)
plot(table)
plot(add,col=add$SS)
plot(add,col=add$SS =4)
plot(add,col=add$SS 4)
#getting indevidual standard deviation
sapply(add[,c(5,6,7)],sd)
sapply(add,class)
#gettting a regression model for the person having cancer or not havoing cancer
fit1 <-
logit.overall = glm(admit ~ gre + gpa + rank,
family = "binomial",
data = GradSchools)
add
family = "binomial"
?family = "binomial"
data = add)
logit.overall = glm(LC ~ FM + SS + BK + AG + YR + CD,
family = "binomial",
data = add)
plot(logit.overall)
table(add$LC/add)
table(add$LC)/nrow(add)
table(add$LC)
ADD
add
table(add$LC,add$BK)
table(add$LC,add$CD,add$YR)
table(add$LC,add$YR)
table(add$LC,add$CD)
logit.overall = lm(LC ~ FM + SS + BK + AG + YR + CD,
family = "binomial",
data = add)
logit.overall = lm(LC ~ FM + SS + BK + AG + YR + CD,
data = add)
table(add$LC)/nrow(add)
table(add$LC)
add
table(add$LC,add$BK)
Summary(logit.overall)
Summary(logit.overall$fitted.values)
logit.overall = lm(LC ~ FM + SS + BK + AG + YR + CD,
data = add)
Summary(logit.overall$fitted.values)
table(add$LC,add$BK)
Summary(logit.overall$fitted.values)
logit.overall = glm(LC ~ FM + SS + BK + AG + YR + CD,
data = add)
logit.overall = glm(LC ~ FM + SS + BK + AG + YR + CD,
family = "binomial",
data = add)
#Residual plot for logistic regression with an added loess smoother; we would
#hope that, on average, the residual values are 0.
scatter.smooth(logit.overall$fit,
residuals(logit.overall, type = "deviance"),
lpars = list(col = "red"),
xlab = "Fitted Probabilities",
ylab = "Deviance Residual Values",
main = "Residual Plot for\nLogistic Regression of Admission Data")
abline(h = 0, lty = 2)
plot(logit.overall$fitted.values)
plot(logit.overall$residuals)
influencePlot(logit.overall)
influence(logit.overall)
Plotinfluence(logit.overall)
Plot.influence(logit.overall)
influencePlot(add$LC)
influencePlot(logit.overall)
influencePlot(logit.overall)
#-Intercept: The log odds of a student getting admitted to a graduate school
#            when they attended a top tier undergraduate school and received a
#            0 on the GRE and a 0 as their GPA is approximately -3.990.
#-GRE: For every additional point a student scores on the GRE, their log odds
#      of being admitted to graduate school increase by approximately 0.002,
#      holding all other variables constant.
#-GPA: For every additional point a student raises their GPA, their log odds of
#      being admitted to graduate school increase by approximately 0.804, holding
#      all other variables constant.
#-Rank: The change in log odds associated with attending an undergraduate school
#       with prestige of rank 2, 3, and 4, as compared to a school with prestige
#       rank 1, is approximately -0.675, -1.340, and -1.552, respectively, holding
#       all other variables constant.
exp(logit.overall$coefficients)
exp(logit.overall$coefficients)
cbind("Log Odds" = logit.overall$coefficients,
"Odds" = exp(logit.overall$coefficients))
plot(logit.overall$fitted.values)
library(car)
influencePlot(logit.overall)
summary(logit.overall)
table(add$LC,add$BK)
exp(logit.overall$coefficients)
df = read.json('train.csv')
epsilon_ber <- 2*rbinom(100,1,p=0.5)-1     # converting the values 0, 1 to -1, 1
x_ber <- cumsum(epsilon_ber)
plot(x_ber, type='l')
# Gaussian random walk
# rbinom with size = 1 is nothing but Bernoulli distribution
epsilon_ber <- 2*rbinom(100,1,p=0.5)-1     # converting the values 0, 1 to -1, 1
x_ber <- cumsum(epsilon_ber)
plot(x_ber, type='l')
# Gaussian random walk
epsilon_norm <- rnorm(100, 0, sd=0.1)
x_norm <- cumsum(epsilon_norm)
plot(x_norm, type='l')
# 2D random walks
# Bernoulli random walk
N<-10000
epsilon1_ber <-  2 * rbinom(N,1,p=0.5) - 1
epsilon2_ber <-  2 * rbinom(N,1,p=0.5) - 1
x_ber <-  cumsum(epsilon1_ber)
y_ber <-  cumsum(epsilon2_ber)
plot(x_ber, y_ber, type='l')
# Gaussian random walk
N<-10000
epsilon1_norm <-  rnorm(N,0,0.1)
epsilon2_norm <-  rnorm(N,0,0.1)
x_norm <-  cumsum(epsilon1_norm)
y_norm <-  cumsum(epsilon2_norm)
plot(x_norm, y_norm, type='l')
# SnP500 index price series
library(quantmod)
sp500<-new.env()
getSymbols('^GSPC',env=sp500, src='yahoo', from=as.Date('1960-01-04'),
to=as.Date('2017-03-17'))
head(sp500$GSPC)
tail(sp500$GSPC)
#header: GSPC.Open GSPC.High GSPC.Low GSPC.Close GSPC.Volume GSPC.Adjusted
class(sp500$GSPC)
sp_prc<-sp500$GSPC[,6]
class(sp_prc)    # xts, zoo      'zoo' stands for Z's ordered observations
# xts can handle irregular period time series as well as regular time series
#1960-01-04         59.91
#2017-03-17       2378.25
# total return    (2372.60-59.91)/59.91 ~ 39.60
# daily return
plot(sp_prc)
ret_sp500_daily<-(sp_prc-lag(sp_prc))/lag(sp_prc)
#or
ret_sp500_daily<-diff(sp_prc)/lag(sp_prc)
plot(ret_sp500_daily)
mean(ret_sp500_daily,na.rm=T) * 252     # annualized return
# 0.0771279 ~ 7.7%
# This excludes the yearly dividends paid out to the investors
# the SP500's dividend yield is currently ~2%, the longer term average is 3-5%.
# total return is >= 10%/year
# to get the time series with dividends, use the ETF ticker = SPY
spy<-new.env()
getSymbols('SPY',env=spy, src='yahoo', from=as.Date('1993-01-29'),
to=as.Date('2017-03-17'))
head(spy$SPY)
spy_prc <- spy$SPY[,6]
ret_spy_daily <- diff(spy_prc)/lag(spy_prc)
mean(ret_spy_daily, na.rm=T) * 252   # ~ 10.6097  vs  8.68% for SPX in the same period
# rollmean
sp_avg20 = rollmean(sp_prc, 20, align='center')
plot(sp_avg20)
sp_avg100 = rollmean(sp_prc, 100, align='center')
plot(sp_avg100)
ret_sp500_daily<-ret_sp500_daily[2:length(ret_sp500_daily)]  # removing the first nan element
ret_sp500_avg20 = rollmean(ret_sp500_daily, 20, align='center')
plot(ret_sp500_avg20)
ret_sp500_avg100 = rollmean(ret_sp500_daily, 100, align='center')
plot(ret_sp500_avg100)       # The 100 trading days moving average is smoother than the 20 days version
# The 1987 Black Monday drops 20% in a day, but its effect is smoothed out after 100-day moving window
# Now the most significant drop becomes the finanical crisis during 2008-2009
# test for stationary
# adf.test
library(tseries)
adf.test(sp_prc)
#Augmented Dickey-Fuller Test
#data:  sp_prc
#Dickey-Fuller = -0.59095, Lag order = 24, p-value = 0.9777
#alternative hypothesis: stationary
# 97.7% probability it is NOT a stationary time series
# This can be seen by the naked eye
adf.test(ret_sp500_daily)
#data:  ret_sp500_daily
#Dickey-Fuller = -24.578, Lag order = 24, p-value = 0.01
#alternative hypothesis: stationary
#Warning message:
#  In adf.test(ret_sp500_daily) : p-value smaller than printed p-value
# About < 1% chance to be a non-stationary time series
sp_logPrc <- log(sp_prc)
logRet_sp500_daily <- diff(sp_logPrc)
logRet_sp500_daily <- logRet_sp500_daily[2:length(logRet_sp500_daily)]
adf.test(logRet_sp500_daily)
#Augmented Dickey-Fuller Test
#data:  logRet_sp500_daily
#Dickey-Fuller = -24.315, Lag order = 24, p-value = 0.01
#alternative hypothesis: stationary
#Warning message:
#  In adf.test(logRet_sp500_daily) : p-value smaller than printed p-value
### Some simulated time series to demonstrate the usage of augumented Fuller-Dickey test
#
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.2
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
adf.test(X)
set.seed(0)
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.95
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
adf.test(X)
# > 46% chance it is a non-stationary time series
# actually such an AR(1) time series is indeed stationary
set.seed(0)
N <- 1000
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.95
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
adf.test(X)
# Increasing N to 1000, p-value drops to <1%
# Conclusion:  The accuracy of the adf.test depends on the sample size
# Do not apply the statistical test blindly
set.seed(0)
N <- 100
sigma <- 0.01
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.95
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
adf.test(X)
N <- 100
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.5
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
acf(X, lag.max=10)  # lag.max = 10 to plot only the 10 terms
acf(X, lag.max=10)  # lag.max = 10 to plot only the 10 terms
N <- 1000
sigma <- 0.1
epsilon = rnorm(N, 0, sigma)
X <- numeric(N)
theta <- 0.5
X[1]  <- 2.0
for (i in 2:N) {
X[i] <- theta * X[i-1] + epsilon[i]
}
acf(X, lag.max=10)  # The noise in the auto-correlation function reduces as N goes large
acf(ret_sp500_daily, lag.max=100)   # The stock index is generally efficient, there is no long term pattern
sp500_vola_daily <-rollapply(ret_sp500_daily, width=100, FUN=sd)
plot(sp500_vola_daily)
setwd("C:/Users/Arjun Singh Yadav/Desktop/Python_R_workshop/Introduction to R")
date: "28 April 2017"
###########################
which R
r
?R
VERSION
?version
R.version()
R.Version()
plot(density(heights), main = "Sample Distribution of Heights")
plot(density(heights), main = "Sample Distribution of Heights")
?rnorm
??rnorm
v = 10:15
v
dim() <- c(1,3)
dim() <- c(1,6)
dim() <- c(1,5)
dim(v) <- c(1,5)
dim(v) <- c(1,4)
dim(v) <- c(1,6)
v
?resf
?read.csv
x = c(1,2,3,4,5)
x[-1]
x[,-1]
iris
?paste
install.packages( "dplyr")
library(dplyr)
mpg
data(mpg)
mpg
data()
data
data = data(mpg)
data
library(ggplot2)
mpg
setwd("C:/Users/Arjun Singh Yadav/Desktop/Python_R_workshop/Introduction to R/Machine Learning")
GradSchools = read.table("data/Graduate Schools.txt")
View(GradSchools)
summary(GradSchools) #Looking at the five number summary information.
GradSchools.isna()
GradSchools.isnull()
?na
summary(GradSchools)
str(GradSchools)
GradSchools$admit=='NA'
GradSchools[GradSchools$admit=='NA']
?isna
?isna()
isnull()
?isnull()
describe(GradSchools)
model = lm(gre ~ rank+gpa,data = GradSchools)
model
plot(bad.model) #Severe violations to the assumptions of linear regression.
bad.model = lm(admit ~ gre + gpa + rank, data = GradSchools)
summary(bad.model) #Looks like everything is significant, so what's bad?
logit.overall = glm(admit ~ gre + gpa + rank,
family = "binomial",
data = GradSchools)
logit.overall
summary(logit.overall)
scatter.smooth(logit.overall$fit,
residuals(logit.overall, type = "deviance"),
lpars = list(col = "red"),
xlab = "Fitted Probabilities",
ylab = "Deviance Residual Values",
main = "Residual Plot for\nLogistic Regression of Admission Data")
abline(h = 0, lty = 2)
library(car)
install.packages('car')
library(car)
library(mpg)
influencePlot(logit.overall) #Can still inspect the influence plot.
?influence
ibrary(StatSmooth)
library(Stats)
library(stats)
influencePlot(logit.overall) #Can still inspect the influence plot.
?influencePlot
exp(logit.overall$coefficients)
cbind("Log Odds" = logit.overall$coefficients,
"Odds" = exp(logit.overall$coefficients))
confint(logit.overall) #For logistic regression objects, the confint() function
logit.norank = glm(admit ~ gre + gpa,
family = "binomial",
data = GradSchools)
reduced.deviance = logit.norank$deviance #Comparing the deviance of the reduced
reduced.df = logit.norank$df.residual    #model (the one without rank) to...
full.deviance = logit.overall$deviance #...the deviance of the full model (the
full.df = logit.overall$df.residual    #one with the rank terms).
pchisq(reduced.deviance - full.deviance,
reduced.df - full.df,
lower.tail = FALSE)
predict(logit.overall, newdata, type = "response")
newdata = with(GradSchools, data.frame(gre = mean(gre),
gpa = mean(gpa),
rank = factor(1:4)))
newdata #Creating a data frame with the average GRE and GPA for each level of
#the rank variable.
predict(logit.overall, newdata) #This gives us the log odds; but we want
#the probabilities.
newdata = with(GradSchools, data.frame(gre = mean(gre),
gpa = mean(gpa),
))
library(ISLR)
Hitters = na.omit(Hitters)
help(Hitters)
grid = 10^seq(5, -2, length = 100)
x = model.matrix(Salary ~ ., Hitters)[, -1] #Dropping the intercept column.
y = Hitters$Salary
#Values of lambda over which to check.
grid = 10^seq(5, -2, length = 100)
#Fitting the ridge regression. Alpha = 0 for ridge regression.
library(glmnet)
install.packages('glmnet')
install.packages('glmnet')
library(glmnet)
library(glmnet)
install.packages(c("assertthat", "curl", "DBI", "gbm", "htmltools", "jsonlite", "markdown", "memoise", "psych", "quantmod", "Rcpp", "rgl", "shiny", "sourcetools", "stringi", "tibble", "tm", "tseries", "viridis", "zoo"))
install.packages(c("assertthat", "curl", "DBI", "gbm", "htmltools", "jsonlite", "markdown", "memoise", "psych", "quantmod", "Rcpp", "rgl", "shiny", "sourcetools", "stringi", "tibble", "tm", "tseries", "viridis", "zoo"))
library(glmnet)
install.packages('glmnet')
library(glmnet)
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
grid = 10^seq(5, -2, length = 100)
#Fitting the ridge regression. Alpha = 0 for ridge regression.
ridge.models = glmnet(x, y, alpha = 0, lambda = grid)
dim(coef(ridge.models)) #20 different coefficients, estimated 100 times --
#once each per lambda value.
coef(ridge.models) #Inspecting the various coefficient estimates.
ridge.models$lambda[80] #Lambda = 0.2595.
coef(ridge.models)[, 80] #Estimates not close to 0.
sqrt(sum(coef(ridge.models)[-1, 80]^2)) #L2 norm is 136.8179.
#What do the estimates look like for a larger value of lambda?
ridge.models$lambda[15] #Lambda = 10,235.31.
coef(ridge.models)[, 15] #Most estimates close to 0.
sqrt(sum(coef(ridge.models)[-1, 15]^2)) #L2 norm is 7.07.
#Visualizing the ridge regression shrinkage.
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
predict(ridge.models, s = 50, type = "coefficients")
set.seed(0)
train = sample(1:nrow(x), 7*nrow(x)/10)
test = (-train)
y.test = y[test]
length(train)/nrow(x)
length(y.test)/nrow(x)
#Let's attempt to fit a ridge regression using some arbitrary value of lambda;
#we still have not yet figured out what the best value of lambda should be!
#We will arbitrarily choose 5. We will now use the training set exclusively.
ridge.models.train = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
ridge.lambda5 = predict(ridge.models.train, s = 5, newx = x[test, ])
mean((ridge.lambda5 - y.test)^2)
#Here, the MSE is approximately 115,541.
#What would happen if we fit a ridge regression with an extremely large value
#of lambda? Essentially, fitting a model with only an intercept:
library(tree)
#Loading the ISLR library in order to use the Carseats dataset.
library(ISLR)
#Making data manipulation easier.
help(Carseats)
attach(Carseats)
#Looking at the variable of interest, Sales.
hist(Sales)
summary(Sales)
#Creating a binary categorical variable High based on the continuous Sales
#variable and adding it to the original data frame.
#variable and adding it to the original data frame.
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
#Fit a tree to the data; note that we are excluding Sales from the formula.
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0) #Yields category names instead of dummy variables.
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.
Carseats.test = Carseats[-train, ] #Test dataset.
High.test = High[-train] #Test response.
#Ftting and visualizing a classification tree to the training data.
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
library(MASS)
help(Boston)
set.seed(0)
train = sample(1:nrow(Boston), 7*nrow(Boston)/10)
#Training the tree to predict the median value of owner-occupied homes (in $1k).
tree.boston = tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty = 0)
set.seed(0)
cv.boston = cv.tree(tree.boston)
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
prune.boston = prune.tree(tree.boston, best = 4)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
library(randomForest)
install.packages('randomForest')
library(randomForest)
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
rf.boston
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
rf.boston
#Varying the number of variables used at each step of the random forest procedure.
set.seed(0)
oob.err = numeric(13)
oob.err
for (mtry in 1:13) {
fit = randomForest(medv ~ ., data = Boston[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
plot(1:13, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
#Fitting 10,000 trees with a depth of 4.
set.seed(0)
boost.boston = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4)
par(mfrow = c(1, 1))
summary(boost.boston)
